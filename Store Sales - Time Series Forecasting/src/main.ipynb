{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:24:19.745969Z",
     "start_time": "2026-01-03T18:24:19.734152Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "69dd3879274e19f1",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Firstly, we need to load the data.",
   "id": "cdd7d46a376aede1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:24:21.456547Z",
     "start_time": "2026-01-03T18:24:19.771507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "oil = pd.read_csv('../data/oil.csv')\n",
    "stores = pd.read_csv('../data/stores.csv')\n",
    "transaction = pd.read_csv('../data/transactions.csv')\n",
    "holidays = pd.read_csv('../data/holidays_events.csv')"
   ],
   "id": "76cb50627f17e7e1",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we need to view the shape and the columns contained in each file / dataframe so we can better recognise how to merge the data, and what we can actually use.",
   "id": "87692e8226d5d677"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:24:21.681792Z",
     "start_time": "2026-01-03T18:24:21.659792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Train data:\\n {train_data.head()}\")\n",
    "print(f\"Test data:\\n {test_data.head()}\")\n",
    "print(f\"Oil data:\\n {oil.head()}\")\n",
    "print(f\"Store data:\\n {stores.head()}\")\n",
    "print(f\"Transaction data:\\n {transaction.head()}\")\n",
    "print(f\"Holidays data:\\n {holidays.head()}\")"
   ],
   "id": "e5305d8d1ac13dc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "    id        date  store_nbr      family  sales  onpromotion\n",
      "0   0  2013-01-01          1  AUTOMOTIVE    0.0            0\n",
      "1   1  2013-01-01          1   BABY CARE    0.0            0\n",
      "2   2  2013-01-01          1      BEAUTY    0.0            0\n",
      "3   3  2013-01-01          1   BEVERAGES    0.0            0\n",
      "4   4  2013-01-01          1       BOOKS    0.0            0\n",
      "Test data:\n",
      "         id        date  store_nbr      family  onpromotion\n",
      "0  3000888  2017-08-16          1  AUTOMOTIVE            0\n",
      "1  3000889  2017-08-16          1   BABY CARE            0\n",
      "2  3000890  2017-08-16          1      BEAUTY            2\n",
      "3  3000891  2017-08-16          1   BEVERAGES           20\n",
      "4  3000892  2017-08-16          1       BOOKS            0\n",
      "Oil data:\n",
      "          date  dcoilwtico\n",
      "0  2013-01-01         NaN\n",
      "1  2013-01-02       93.14\n",
      "2  2013-01-03       92.97\n",
      "3  2013-01-04       93.12\n",
      "4  2013-01-07       93.20\n",
      "Store data:\n",
      "    store_nbr           city                           state type  cluster\n",
      "0          1          Quito                       Pichincha    D       13\n",
      "1          2          Quito                       Pichincha    D       13\n",
      "2          3          Quito                       Pichincha    D        8\n",
      "3          4          Quito                       Pichincha    D        9\n",
      "4          5  Santo Domingo  Santo Domingo de los Tsachilas    D        4\n",
      "Transaction data:\n",
      "          date  store_nbr  transactions\n",
      "0  2013-01-01         25           770\n",
      "1  2013-01-02          1          2111\n",
      "2  2013-01-02          2          2358\n",
      "3  2013-01-02          3          3487\n",
      "4  2013-01-02          4          1922\n",
      "Holidays data:\n",
      "          date     type    locale locale_name                    description  \\\n",
      "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
      "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
      "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
      "3  2012-04-14  Holiday     Local    Libertad      Cantonizacion de Libertad   \n",
      "4  2012-04-21  Holiday     Local    Riobamba      Cantonizacion de Riobamba   \n",
      "\n",
      "   transferred  \n",
      "0        False  \n",
      "1        False  \n",
      "2        False  \n",
      "3        False  \n",
      "4        False  \n"
     ]
    }
   ],
   "execution_count": 228
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So, the distinctive data we can use, so that we can get the best possible prediction results, is if we use as an index the store and the date where it is possible. We now need to merge the date and make sure we have a consistent time frame, so we want our input data to have a 1 day difference, between 2 data rows, we have to make sure our data is ordered by date.",
   "id": "43d20448b8e18ff4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:24:22.977887Z",
     "start_time": "2026-01-03T18:24:21.714308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "oil['date'] = pd.to_datetime(oil['date'])\n",
    "transaction['date'] = pd.to_datetime(transaction['date'])\n",
    "holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "\n",
    "train_data = train_data.merge(\n",
    "    oil, how='left', on='date'\n",
    ")\n",
    "\n",
    "\n",
    "train_data = train_data.merge(\n",
    "    stores,\n",
    "    how='left',\n",
    "    on=['store_nbr']\n",
    ")\n",
    "\n",
    "test_data = test_data.merge(\n",
    "    oil, how='left', on='date'\n",
    ")\n",
    "\n",
    "test_data = test_data.merge(\n",
    "    stores,\n",
    "    how='left',\n",
    "    on=['store_nbr']\n",
    ")"
   ],
   "id": "6be7d29114089aa8",
   "outputs": [],
   "execution_count": 229
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We did not merge all the available data yet. If we pay attention to the dataframes, we notice that the transaction data have no available data about the dates we want to predict the value of, if we just try to merge those tables, in the test data, we will just a get a column filled with nan / null values, the only thing this can do is throw off our predictions. The data we get from this dataset though is highly valuable, so we cannot just not use it. What we can actually do is work with lags and window frames. The logic for that is to actually match future dates with past values. For the lag data, we can agree on a consistent lag time frame, and just use the say average of the last 7 days as the prediction for the current date.",
   "id": "b5c7beef47efb64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:24:29.001737Z",
     "start_time": "2026-01-03T18:24:23.170118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# âœ… CORRECT: Use transactions (known in advance) to predict sales\n",
    "train_data = train_data.merge(transaction, how='left', on=['date', 'store_nbr'])\n",
    "test_data = test_data.merge(transaction, how='left', on=['date', 'store_nbr'])\n",
    "\n",
    "# Combine for lag feature creation\n",
    "combined = pd.concat([train_data, test_data], ignore_index=True)\n",
    "combined = combined.sort_values(['store_nbr', 'family', 'date'])\n",
    "\n",
    "# Create transaction lags (NOT sales lags!)\n",
    "combined['transactions_lag_7'] = (\n",
    "    combined\n",
    "    .groupby(['store_nbr', 'family'])['transactions']\n",
    "    .shift(7)\n",
    ")\n",
    "\n",
    "combined['transactions_roll_mean_7'] = (\n",
    "    combined\n",
    "    .groupby(['store_nbr', 'family'])['transactions']\n",
    "    .shift(1)\n",
    "    .rolling(7, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Split back\n",
    "train_data = combined[combined['sales'].notna()].copy()\n",
    "test_data = combined[combined['sales'].isna()].copy()\n",
    "combined['transactions_lag_7'] = combined.groupby(['store_nbr'])['transactions'].shift(7)\n"
   ],
   "id": "3d093bae8d179bd7",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So, the last dataset we have to combine to our testing and training set, is the holiday dataset. To do that we have we have to create a column for each type of holiday and just assign true / false values according to the dataset. One thing we have to keep in mind is if the holiday was transferred, if it was then there is no reason for that info to be passed on the features.",
   "id": "f334765dee883efd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:24:34.847764Z",
     "start_time": "2026-01-03T18:24:29.328943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "national_holidays = holidays[(holidays['locale'] == 'National') & (holidays['transferred'] == False)]\n",
    "regional_holidays = holidays[(holidays['locale'] == 'Regional') & (holidays['transferred'] == False)]\n",
    "local_holidays = holidays[(holidays['locale'] == 'Local') & (holidays['transferred'] == False)]\n",
    "\n",
    "test_data['is_national_holiday'] = test_data['date'].isin(national_holidays)\n",
    "train_data['is_national_holiday'] = train_data['date'].isin(national_holidays)\n",
    "\n",
    "test_data['is_regional_holiday'] = (\n",
    "    test_data\n",
    "    .merge(\n",
    "        regional_holidays[['date', 'locale_name']].assign(is_regional_holiday=True),\n",
    "        left_on=['date', 'state'],\n",
    "        right_on=['date', 'locale_name'],\n",
    "        how='left'\n",
    "    )['is_regional_holiday']\n",
    "    .fillna(False)\n",
    ")\n",
    "train_data['is_regional_holiday'] = (\n",
    "    train_data\n",
    "    .merge(\n",
    "        regional_holidays[['date', 'locale_name']].assign(is_regional_holiday=True),\n",
    "        left_on=['date', 'state'],\n",
    "        right_on=['date', 'locale_name'],\n",
    "        how='left'\n",
    "    )['is_regional_holiday']\n",
    "    .fillna(False)\n",
    ")\n",
    "\n",
    "test_data['is_local_holiday'] = (\n",
    "    test_data\n",
    "    .merge(\n",
    "        local_holidays[['date', 'locale_name']].assign(is_local_holiday=True),\n",
    "        left_on=['date', 'state'],\n",
    "        right_on=['date', 'locale_name'],\n",
    "        how='left'\n",
    "    )['is_local_holiday']\n",
    "    .fillna(False)\n",
    ")\n",
    "train_data['is_local_holiday'] = (\n",
    "    train_data\n",
    "    .merge(\n",
    "        local_holidays[['date', 'locale_name']].assign(is_local_holiday=True),\n",
    "        left_on=['date', 'state'],\n",
    "        right_on=['date', 'locale_name'],\n",
    "        how='left'\n",
    "    )['is_local_holiday']\n",
    "    .fillna(False)\n",
    ")\n",
    "\n",
    "print(f\"Test data:\\n {test_data.head()}\")\n",
    "print(f\"Train data:\\n {train_data.head()}\")"
   ],
   "id": "f1096bf22c81f764",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ece21\\AppData\\Local\\Temp\\ipykernel_10708\\2718324608.py:16: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .fillna(False)\n",
      "C:\\Users\\ece21\\AppData\\Local\\Temp\\ipykernel_10708\\2718324608.py:26: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .fillna(False)\n",
      "C:\\Users\\ece21\\AppData\\Local\\Temp\\ipykernel_10708\\2718324608.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n",
      "               id       date  store_nbr      family  sales  onpromotion  \\\n",
      "3000888  3000888 2017-08-16          1  AUTOMOTIVE    NaN            0   \n",
      "3002670  3002670 2017-08-17          1  AUTOMOTIVE    NaN            0   \n",
      "3004452  3004452 2017-08-18          1  AUTOMOTIVE    NaN            0   \n",
      "3006234  3006234 2017-08-19          1  AUTOMOTIVE    NaN            0   \n",
      "3008016  3008016 2017-08-20          1  AUTOMOTIVE    NaN            0   \n",
      "\n",
      "         dcoilwtico   city      state type  cluster  transactions  \\\n",
      "3000888       46.80  Quito  Pichincha    D       13           NaN   \n",
      "3002670       47.07  Quito  Pichincha    D       13           NaN   \n",
      "3004452       48.59  Quito  Pichincha    D       13           NaN   \n",
      "3006234         NaN  Quito  Pichincha    D       13           NaN   \n",
      "3008016         NaN  Quito  Pichincha    D       13           NaN   \n",
      "\n",
      "         transactions_lag_7  transactions_roll_mean_7  is_national_holiday  \\\n",
      "3000888              1766.0               1278.000000                False   \n",
      "3002670              1764.0               1196.666667                False   \n",
      "3004452               570.0               1083.200000                False   \n",
      "3006234              1004.0               1211.500000                False   \n",
      "3008016               416.0               1280.666667                False   \n",
      "\n",
      "        is_regional_holiday is_local_holiday  \n",
      "3000888                 NaN              NaN  \n",
      "3002670                 NaN              NaN  \n",
      "3004452                 NaN              NaN  \n",
      "3006234                 NaN              NaN  \n",
      "3008016                 NaN              NaN  \n",
      "Train data:\n",
      "         id       date  store_nbr      family  sales  onpromotion  dcoilwtico  \\\n",
      "0        0 2013-01-01          1  AUTOMOTIVE    0.0            0         NaN   \n",
      "1782  1782 2013-01-02          1  AUTOMOTIVE    2.0            0       93.14   \n",
      "3564  3564 2013-01-03          1  AUTOMOTIVE    3.0            0       92.97   \n",
      "5346  5346 2013-01-04          1  AUTOMOTIVE    3.0            0       93.12   \n",
      "7128  7128 2013-01-05          1  AUTOMOTIVE    5.0            0         NaN   \n",
      "\n",
      "       city      state type  cluster  transactions  transactions_lag_7  \\\n",
      "0     Quito  Pichincha    D       13           NaN                 NaN   \n",
      "1782  Quito  Pichincha    D       13        2111.0                 NaN   \n",
      "3564  Quito  Pichincha    D       13        1833.0                 NaN   \n",
      "5346  Quito  Pichincha    D       13        1863.0                 NaN   \n",
      "7128  Quito  Pichincha    D       13        1509.0                 NaN   \n",
      "\n",
      "      transactions_roll_mean_7  is_national_holiday  is_regional_holiday  \\\n",
      "0                          NaN                False                False   \n",
      "1782                       NaN                False                False   \n",
      "3564               2111.000000                False                False   \n",
      "5346               1972.000000                False                False   \n",
      "7128               1935.666667                False                False   \n",
      "\n",
      "      is_local_holiday  \n",
      "0                False  \n",
      "1782             False  \n",
      "3564             False  \n",
      "5346             False  \n",
      "7128             False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ece21\\AppData\\Local\\Temp\\ipykernel_10708\\2718324608.py:47: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .fillna(False)\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also have info about an earthquake that happened in Ecuador on April 16, 2016, so we can also add that as an additional column, we also know that workers are paid every 15th and last day of the month, so we can also get some info out of that.",
   "id": "da56cb90c356c2b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:24:35.676848Z",
     "start_time": "2026-01-03T18:24:34.980690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data['is_natural_disaster'] = (\n",
    "    (train_data['date'] == pd.to_datetime('2016-04-16')) &\n",
    "    (train_data['city'] == 'Ecuador')\n",
    ")\n",
    "\n",
    "test_data['is_natural_disaster'] = (\n",
    "    (test_data['date'] == pd.to_datetime('2016-04-16')) &\n",
    "    (test_data['city'] == 'Ecuador')\n",
    ")\n",
    "\n",
    "\n",
    "train_data['is_payday'] = (\n",
    "    (pd.to_datetime(train_data['date']).dt.day == 15) |\n",
    "    (pd.to_datetime(train_data['date']).dt.day ==\n",
    "     pd.to_datetime(train_data['date']).dt.days_in_month)\n",
    ")\n",
    "\n",
    "test_data['is_payday'] = (\n",
    "    (pd.to_datetime(test_data['date']).dt.day == 15) |\n",
    "    (pd.to_datetime(test_data['date']).dt.day ==\n",
    "     pd.to_datetime(test_data['date']).dt.days_in_month)\n",
    ")"
   ],
   "id": "6c505661c130c46e",
   "outputs": [],
   "execution_count": 232
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now we are ready to train our model and get predictions",
   "id": "c32355acf29fda11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:28:39.739380Z",
     "start_time": "2026-01-03T18:24:35.709886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "train_data = train_data.drop(columns=['date', 'city', 'state', 'type'])\n",
    "test_data = test_data.drop(columns=['date', 'city', 'state', 'type'])\n",
    "\n",
    "categorical_features = train_data.select_dtypes(exclude=['bool', 'number', 'datetime']).columns\n",
    "\n",
    "print(categorical_features)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_features)\n",
    "test_data = pd.get_dummies(test_data, columns=categorical_features)\n",
    "\n",
    "y_train = train_data['sales']\n",
    "X_train = train_data.drop('sales', axis=1)\n",
    "X_test = test_data\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3,\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    task_type='CPU',\n",
    "    loss_function='RMSE'\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': X_test['id'],\n",
    "    'sales': y_pred\n",
    "})\n",
    "\n",
    "submission.to_csv('../data/catboost_submission_final.csv', index=False)"
   ],
   "id": "fc00fc164c51b408",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['family'], dtype='object')\n",
      "Training model...\n",
      "0:\tlearn: 1059.3484663\ttotal: 293ms\tremaining: 4m 52s\n",
      "100:\tlearn: 373.7553261\ttotal: 24.9s\tremaining: 3m 41s\n",
      "200:\tlearn: 333.3221804\ttotal: 49.1s\tremaining: 3m 15s\n",
      "300:\tlearn: 309.7873226\ttotal: 1m 12s\tremaining: 2m 49s\n",
      "400:\tlearn: 294.0422762\ttotal: 1m 36s\tremaining: 2m 24s\n",
      "500:\tlearn: 283.2038853\ttotal: 1m 59s\tremaining: 1m 59s\n",
      "600:\tlearn: 274.7507314\ttotal: 2m 24s\tremaining: 1m 35s\n",
      "700:\tlearn: 267.8579714\ttotal: 2m 46s\tremaining: 1m 11s\n",
      "800:\tlearn: 261.4140938\ttotal: 3m 10s\tremaining: 47.2s\n",
      "900:\tlearn: 256.1646113\ttotal: 3m 35s\tremaining: 23.7s\n",
      "999:\tlearn: 251.4467666\ttotal: 3m 59s\tremaining: 0us\n"
     ]
    }
   ],
   "execution_count": 233
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
